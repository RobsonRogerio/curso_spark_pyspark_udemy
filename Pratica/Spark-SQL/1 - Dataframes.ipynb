{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "adfdb611-d07d-4af9-85fc-2275c5b449e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/08/04 15:39:49 WARN Utils: Your hostname, LAPTOP-CO0U45G1, resolves to a loopback address: 127.0.1.1; using 172.20.235.103 instead (on interface eth0)\n",
      "25/08/04 15:39:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/04 15:39:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/04 15:39:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/08/04 15:39:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/08/04 15:39:58 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c891aff8-2f5b-446f-b783-56901a9bbc8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Teste básico do Spark\n",
    "spark.range(5).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c087efcc-a197-4fac-98b2-e6bf61a0fc16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  Nome|Idade|\n",
      "+------+-----+\n",
      "|Robson|   30|\n",
      "| Maria|   25|\n",
      "|  João|   35|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testando PySpark\n",
    "dados = [(\"Robson\", 30), (\"Maria\", 25), (\"João\", 35)]\n",
    "colunas = [\"Nome\", \"Idade\"]\n",
    "df = spark.createDataFrame(dados, colunas)\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9956852-4ea2-440c-b402-dbd81d40aa91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|  Nome|Idade|\n",
      "+------+-----+\n",
      "|Robson|   30|\n",
      "| Maria|   25|\n",
      "|  João|   35|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testando PySpark\n",
    "dados = [(\"Robson\", 30), (\"Maria\", 25), (\"João\", 35)]\n",
    "schema = \"Nome STRING, Idade INT\"\n",
    "df = spark.createDataFrame(dados, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a133b2-ed93-4608-bdbd-780b1a179ac2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|  Nome|\n",
      "+------+\n",
      "|Robson|\n",
      "| Maria|\n",
      "|  João|\n",
      "+------+\n",
      "\n",
      "+------+-----+\n",
      "|  Nome|Idade|\n",
      "+------+-----+\n",
      "|Robson|   30|\n",
      "| Maria|   25|\n",
      "|  João|   35|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----------+\n",
      "|  Nome|(Idade + 1)|\n",
      "+------+-----------+\n",
      "|Robson|         31|\n",
      "| Maria|         26|\n",
      "|  João|         36|\n",
      "+------+-----------+\n",
      "\n",
      "+----+-----+\n",
      "|Nome|Idade|\n",
      "+----+-----+\n",
      "|João|   35|\n",
      "+----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|Idade|count|\n",
      "+-----+-----+\n",
      "|   30|    1|\n",
      "|   25|    1|\n",
      "|   35|    1|\n",
      "+-----+-----+\n",
      "\n",
      "+------+-----+\n",
      "|  Nome|Idade|\n",
      "+------+-----+\n",
      "|  João|   35|\n",
      "|Robson|   30|\n",
      "| Maria|   25|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Nome\").show()\n",
    "df.select(\"Nome\", \"Idade\").show()\n",
    "df.select(\"Nome\", df.Idade + 1).show()\n",
    "df.filter(df.Idade > 30).show()\n",
    "df.groupBy(\"Idade\").count().show()\n",
    "df.orderBy(df.Idade.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27f3b846-1224-4577-bb3c-78451d0cf7cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('Nome', StringType(), True), StructField('Idade', IntegerType(), True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3efb58c8-4e3a-484c-9d81-564edc0b613a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Nome: string (nullable = true)\n",
      " |-- Idade: integer (nullable = true)\n",
      "\n",
      "+-------+------+-----+\n",
      "|summary|  Nome|Idade|\n",
      "+-------+------+-----+\n",
      "|  count|     3|    3|\n",
      "|   mean|  NULL| 30.0|\n",
      "| stddev|  NULL|  5.0|\n",
      "|    min|  João|   25|\n",
      "|    max|Robson|   35|\n",
      "+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "240fc5fe-8280-4ea1-a085-cb91e8ac9e1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
       "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1535\u001b[0m, in \u001b[0;36mSparkConnectClient._analyze\u001b[0;34m(self, method, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m attempt:\n",
       "\u001b[0;32m-> 1535\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stub\u001b[38;5;241m.\u001b[39mAnalyzePlan(req, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata())\n",
       "\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_response_integrity(resp)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n",
       "\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n",
       "\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n",
       "\u001b[1;32m    270\u001b[0m     request: Any,\n",
       "\u001b[0;32m   (...)\u001b[0m\n",
       "\u001b[1;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n",
       "\u001b[1;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
       "\u001b[0;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_call(\n",
       "\u001b[1;32m    278\u001b[0m         request,\n",
       "\u001b[1;32m    279\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n",
       "\u001b[1;32m    280\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n",
       "\u001b[1;32m    281\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcredentials,\n",
       "\u001b[1;32m    282\u001b[0m         wait_for_ready\u001b[38;5;241m=\u001b[39mwait_for_ready,\n",
       "\u001b[1;32m    283\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n",
       "\u001b[1;32m    284\u001b[0m     )\n",
       "\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n",
       "\u001b[1;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n",
       "\u001b[1;32m    330\u001b[0m     continuation, client_call_details, request\n",
       "\u001b[1;32m    331\u001b[0m )\n",
       "\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n",
       "\u001b[1;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n",
       "\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n",
       "\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thunk(new_method)\u001b[38;5;241m.\u001b[39mwith_call(\n",
       "\u001b[1;32m    316\u001b[0m         request,\n",
       "\u001b[1;32m    317\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mnew_timeout,\n",
       "\u001b[1;32m    318\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mnew_metadata,\n",
       "\u001b[1;32m    319\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mnew_credentials,\n",
       "\u001b[1;32m    320\u001b[0m         wait_for_ready\u001b[38;5;241m=\u001b[39mnew_wait_for_ready,\n",
       "\u001b[1;32m    321\u001b[0m         compression\u001b[38;5;241m=\u001b[39mnew_compression,\n",
       "\u001b[1;32m    322\u001b[0m     )\n",
       "\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n",
       "\u001b[1;32m   1192\u001b[0m (\n",
       "\u001b[1;32m   1193\u001b[0m     state,\n",
       "\u001b[1;32m   1194\u001b[0m     call,\n",
       "\u001b[1;32m   1195\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n",
       "\u001b[1;32m   1196\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n",
       "\u001b[1;32m   1197\u001b[0m )\n",
       "\u001b[0;32m-> 1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n",
       "\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
       "\n",
       "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n",
       "\tstatus = StatusCode.UNAVAILABLE\n",
       "\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n",
       "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\", grpc_status:14, created_time:\"2025-08-04T18:29:44.697736286+00:00\"}\"\n",
       ">\n",
       "\n",
       "The above exception was the direct cause of the following exception:\n",
       "\n",
       "\u001b[0;31mRetriesExceeded\u001b[0m                           Traceback (most recent call last)\n",
       "File \u001b[0;32m<command-8931851849988884>, line 1\u001b[0m\n",
       "\u001b[0;32m----> 1\u001b[0m display(spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Volumes/workspace/spark_pyspark_udemy/curso_spark_pyspark_udemy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/display.py:142\u001b[0m, in \u001b[0;36mDisplay.display\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n",
       "\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# This version is for Serverless + Spark Connect dogfooding.\u001b[39;00m\n",
       "\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark_connect_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, ConnectDataFrame):\n",
       "\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_connect_table(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, ConnectDataFrame):\n",
       "\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39misStreaming:\n",
       "\n",
       "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/display.py:103\u001b[0m, in \u001b[0;36mDisplay.display_connect_table\u001b[0;34m(self, df, **kwargs)\u001b[0m\n",
       "\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
       "\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\n",
       "\u001b[1;32m    100\u001b[0m         e\n",
       "\u001b[1;32m    101\u001b[0m     )(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython shell encountered an error or was missing data, please restart the notebook or contact Databricks support\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
       "\u001b[1;32m    102\u001b[0m       ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
       "\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39misStreaming:\n",
       "\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcf_helper\u001b[38;5;241m.\u001b[39mdisplay_streaming_dataframe(df, config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming_listener,\n",
       "\u001b[1;32m    105\u001b[0m                                                \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
       "\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
       "\n",
       "File \u001b[0;32m/usr/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n",
       "\u001b[1;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n",
       "\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n",
       "\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n",
       "\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[1;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2017\u001b[0m, in \u001b[0;36mDataFrame.isStreaming\u001b[0;34m(self)\u001b[0m\n",
       "\u001b[1;32m   2014\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n",
       "\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misStreaming\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
       "\u001b[1;32m   2016\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mto_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n",
       "\u001b[0;32m-> 2017\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39m_analyze(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_streaming\u001b[39m\u001b[38;5;124m\"\u001b[39m, plan\u001b[38;5;241m=\u001b[39mquery)\u001b[38;5;241m.\u001b[39mis_streaming\n",
       "\u001b[1;32m   2018\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[1;32m   2019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1540\u001b[0m, in \u001b[0;36mSparkConnectClient._analyze\u001b[0;34m(self, method, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid state during retry exception handling.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
       "\u001b[0;32m-> 1540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n",
       "\u001b[1;32m   2049\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
       "\u001b[1;32m   2050\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectException(\n",
       "\u001b[1;32m   2051\u001b[0m                 error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO_ACTIVE_SESSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m()\n",
       "\u001b[1;32m   2052\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
       "\u001b[0;32m-> 2053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
       "\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
       "\u001b[1;32m   2055\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1533\u001b[0m, in \u001b[0;36mSparkConnectClient._analyze\u001b[0;34m(self, method, **kwargs)\u001b[0m\n",
       "\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n",
       "\u001b[1;32m   1526\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSUPPORTED_OPERATION\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n",
       "\u001b[1;32m   1527\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n",
       "\u001b[1;32m   1528\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation\u001b[39m\u001b[38;5;124m\"\u001b[39m: method,\n",
       "\u001b[1;32m   1529\u001b[0m         },\n",
       "\u001b[1;32m   1530\u001b[0m     )\n",
       "\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
       "\u001b[0;32m-> 1533\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrying():\n",
       "\u001b[1;32m   1534\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m attempt:\n",
       "\u001b[1;32m   1535\u001b[0m             resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stub\u001b[38;5;241m.\u001b[39mAnalyzePlan(req, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata())\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001b[0m, in \u001b[0;36mRetrying.__iter__\u001b[0;34m(self)\u001b[0m\n",
       "\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(\u001b[38;5;28mself\u001b[39m)\n",
       "\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done:\n",
       "\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait()\n",
       "\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(\u001b[38;5;28mself\u001b[39m)\n",
       "\n",
       "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001b[0m, in \u001b[0;36mRetrying._wait\u001b[0;34m(self)\u001b[0m\n",
       "\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Exceeded retries\u001b[39;00m\n",
       "\u001b[1;32m    279\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven up on retrying. error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(exception)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
       "\u001b[0;32m--> 280\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m RetriesExceeded(error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETRIES_EXCEEDED\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m{}) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexception\u001b[39;00m\n",
       "\n",
       "\u001b[0;31mRetriesExceeded\u001b[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "RetriesExceeded",
        "evalue": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "metadata": {
        "errorSummary": "[RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "RETRIES_EXCEEDED",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": null,
        "sqlState": null,
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1535\u001b[0m, in \u001b[0;36mSparkConnectClient._analyze\u001b[0;34m(self, method, **kwargs)\u001b[0m\n\u001b[1;32m   1534\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m attempt:\n\u001b[0;32m-> 1535\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stub\u001b[38;5;241m.\u001b[39mAnalyzePlan(req, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata())\n\u001b[1;32m   1536\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verify_response_integrity(resp)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:277\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    270\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     compression: Optional[grpc\u001b[38;5;241m.\u001b[39mCompression] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m--> 277\u001b[0m     response, ignored_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_with_call(\n\u001b[1;32m    278\u001b[0m         request,\n\u001b[1;32m    279\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    280\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mmetadata,\n\u001b[1;32m    281\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mcredentials,\n\u001b[1;32m    282\u001b[0m         wait_for_ready\u001b[38;5;241m=\u001b[39mwait_for_ready,\n\u001b[1;32m    283\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    284\u001b[0m     )\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:332\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    329\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interceptor\u001b[38;5;241m.\u001b[39mintercept_unary_unary(\n\u001b[1;32m    330\u001b[0m     continuation, client_call_details, request\n\u001b[1;32m    331\u001b[0m )\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m call\u001b[38;5;241m.\u001b[39mresult(), call\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:440\u001b[0m, in \u001b[0;36m_InactiveRpcError.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"See grpc.Future.result.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 440\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_interceptor.py:315\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._with_call.<locals>.continuation\u001b[0;34m(new_details, request)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     response, call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_thunk(new_method)\u001b[38;5;241m.\u001b[39mwith_call(\n\u001b[1;32m    316\u001b[0m         request,\n\u001b[1;32m    317\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mnew_timeout,\n\u001b[1;32m    318\u001b[0m         metadata\u001b[38;5;241m=\u001b[39mnew_metadata,\n\u001b[1;32m    319\u001b[0m         credentials\u001b[38;5;241m=\u001b[39mnew_credentials,\n\u001b[1;32m    320\u001b[0m         wait_for_ready\u001b[38;5;241m=\u001b[39mnew_wait_for_ready,\n\u001b[1;32m    321\u001b[0m         compression\u001b[38;5;241m=\u001b[39mnew_compression,\n\u001b[1;32m    322\u001b[0m     )\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _UnaryOutcome(response, call)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1198\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.with_call\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1192\u001b[0m (\n\u001b[1;32m   1193\u001b[0m     state,\n\u001b[1;32m   1194\u001b[0m     call,\n\u001b[1;32m   1195\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1196\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1197\u001b[0m )\n\u001b[0;32m-> 1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/grpc/_channel.py:1006\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
        "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"failed to connect to all addresses; last error: UNKNOWN: ipv4:127.0.0.1:7073: Failed to connect to remote host: Timeout occurred: FD Shutdown\", grpc_status:14, created_time:\"2025-08-04T18:29:44.697736286+00:00\"}\"\n>",
        "\nThe above exception was the direct cause of the following exception:\n",
        "\u001b[0;31mRetriesExceeded\u001b[0m                           Traceback (most recent call last)",
        "File \u001b[0;32m<command-8931851849988884>, line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m display(spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mtext(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Volumes/workspace/spark_pyspark_udemy/curso_spark_pyspark_udemy\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
        "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/display.py:142\u001b[0m, in \u001b[0;36mDisplay.display\u001b[0;34m(self, input, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# This version is for Serverless + Spark Connect dogfooding.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspark_connect_enabled \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, ConnectDataFrame):\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay_connect_table(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, ConnectDataFrame):\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39misStreaming:\n",
        "File \u001b[0;32m/databricks/python_shell/lib/dbruntime/display.py:103\u001b[0m, in \u001b[0;36mDisplay.display_connect_table\u001b[0;34m(self, df, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\n\u001b[1;32m    100\u001b[0m         e\n\u001b[1;32m    101\u001b[0m     )(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython shell encountered an error or was missing data, please restart the notebook or contact Databricks support\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m       ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m df\u001b[38;5;241m.\u001b[39misStreaming:\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcf_helper\u001b[38;5;241m.\u001b[39mdisplay_streaming_dataframe(df, config, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstreaming_listener,\n\u001b[1;32m    105\u001b[0m                                                \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
        "File \u001b[0;32m/usr/lib/python3.11/functools.py:1001\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, instance, owner)\u001b[0m\n\u001b[1;32m    999\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[1;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[0;32m-> 1001\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1003\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/dataframe.py:2017\u001b[0m, in \u001b[0;36mDataFrame.isStreaming\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2014\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misStreaming\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   2016\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mto_proto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient)\n\u001b[0;32m-> 2017\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_session\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39m_analyze(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_streaming\u001b[39m\u001b[38;5;124m\"\u001b[39m, plan\u001b[38;5;241m=\u001b[39mquery)\u001b[38;5;241m.\u001b[39mis_streaming\n\u001b[1;32m   2018\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1540\u001b[0m, in \u001b[0;36mSparkConnectClient._analyze\u001b[0;34m(self, method, **kwargs)\u001b[0m\n\u001b[1;32m   1538\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid state during retry exception handling.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1539\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m-> 1540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_error(error)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2053\u001b[0m, in \u001b[0;36mSparkConnectClient._handle_error\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m   2049\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke RPC\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclosed\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n\u001b[1;32m   2050\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectException(\n\u001b[1;32m   2051\u001b[0m                 error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNO_ACTIVE_SESSION\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m   2052\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2053\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[1;32m   2054\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   2055\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthread_local\u001b[38;5;241m.\u001b[39minside_error_handling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1533\u001b[0m, in \u001b[0;36mSparkConnectClient._analyze\u001b[0;34m(self, method, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[1;32m   1526\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUNSUPPORTED_OPERATION\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1527\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   1528\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moperation\u001b[39m\u001b[38;5;124m\"\u001b[39m: method,\n\u001b[1;32m   1529\u001b[0m         },\n\u001b[1;32m   1530\u001b[0m     )\n\u001b[1;32m   1532\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1533\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrying():\n\u001b[1;32m   1534\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m attempt:\n\u001b[1;32m   1535\u001b[0m             resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stub\u001b[38;5;241m.\u001b[39mAnalyzePlan(req, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata())\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:295\u001b[0m, in \u001b[0;36mRetrying.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_done:\n\u001b[0;32m--> 295\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait()\n\u001b[1;32m    296\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m AttemptManager(\u001b[38;5;28mself\u001b[39m)\n",
        "File \u001b[0;32m/databricks/python/lib/python3.11/site-packages/pyspark/sql/connect/client/retries.py:280\u001b[0m, in \u001b[0;36mRetrying._wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Exceeded retries\u001b[39;00m\n\u001b[1;32m    279\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGiven up on retrying. error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(exception)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m RetriesExceeded(error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRETRIES_EXCEEDED\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_parameters\u001b[38;5;241m=\u001b[39m{}) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexception\u001b[39;00m\n",
        "\u001b[0;31mRetriesExceeded\u001b[0m: [RETRIES_EXCEEDED] The maximum number of retries has been exceeded."
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(spark.read.text(\"/Volumes/workspace/spark_pyspark_udemy/curso_spark_pyspark_udemy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5c30381-b6cf-4354-b101-44dda68d7432",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+-------------+------+----------+\n",
      "| id|               nome|status|       cidade|vendas|      data|\n",
      "+---+-------------------+------+-------------+------+----------+\n",
      "|  1|   Carminda Pestana| Ativo|  Santa Maria|    23|2020-08-11|\n",
      "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
      "|  3|   Emídio Dornelles| Ativo| Porto Alegre|    34|2020-02-05|\n",
      "|  4|Felisbela Dornelles| Ativo| Porto Alegre|    36|2020-02-05|\n",
      "|  5|     Graça Ornellas| Ativo| Porto Alegre|    12|2020-02-05|\n",
      "|  6|   Matilde Rebouças| Ativo| Porto Alegre|    22|2019-01-05|\n",
      "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|    45|2019-10-05|\n",
      "|  8|      Roque Vásquez| Ativo| Porto Alegre|    65|2020-03-05|\n",
      "|  9|      Uriel Queiroz| Ativo| Porto Alegre|    54|2018-05-05|\n",
      "| 10|   Viviana Sequeira| Ativo| Porto Alegre|     0|2020-09-05|\n",
      "+---+-------------------+------+-------------+------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# /Workspace/Users/robson.rogerio@gmail.com/despachantes.csv\n",
    "\n",
    "arqschema = 'id int, nome string, status string, cidade string, vendas int, data string'\n",
    "despachantes = spark.read.csv('/home/robson/curso_spark_pyspark_udemy/despachantes.csv', schema=arqschema)\n",
    "despachantes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b105c60-6507-4a13-a780-9c9f0c25def9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|_c0|                _c1|  _c2|          _c3|_c4|       _c5|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "|  1|   Carminda Pestana|Ativo|  Santa Maria| 23|2020-08-11|\n",
      "|  2|    Deolinda Vilela|Ativo|Novo Hamburgo| 34|2020-03-05|\n",
      "|  3|   Emídio Dornelles|Ativo| Porto Alegre| 34|2020-02-05|\n",
      "|  4|Felisbela Dornelles|Ativo| Porto Alegre| 36|2020-02-05|\n",
      "|  5|     Graça Ornellas|Ativo| Porto Alegre| 12|2020-02-05|\n",
      "|  6|   Matilde Rebouças|Ativo| Porto Alegre| 22|2019-01-05|\n",
      "|  7|    Noêmia   Orriça|Ativo|  Santa Maria| 45|2019-10-05|\n",
      "|  8|      Roque Vásquez|Ativo| Porto Alegre| 65|2020-03-05|\n",
      "|  9|      Uriel Queiroz|Ativo| Porto Alegre| 54|2018-05-05|\n",
      "| 10|   Viviana Sequeira|Ativo| Porto Alegre|  0|2020-09-05|\n",
      "+---+-------------------+-----+-------------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "desp_autoschema = spark.read.load('/home/robson/curso_spark_pyspark_udemy/despachantes.csv', header=False, format='csv', sep=',', inferSchema=True)\n",
    "desp_autoschema.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1231c3a-8952-4c22-bf2f-c86b181b01a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+\n",
      "| id|               nome|vendas|\n",
      "+---+-------------------+------+\n",
      "|  1|   Carminda Pestana|    23|\n",
      "|  2|    Deolinda Vilela|    34|\n",
      "|  3|   Emídio Dornelles|    34|\n",
      "|  4|Felisbela Dornelles|    36|\n",
      "|  6|   Matilde Rebouças|    22|\n",
      "|  7|    Noêmia   Orriça|    45|\n",
      "|  8|      Roque Vásquez|    65|\n",
      "|  9|      Uriel Queiroz|    54|\n",
      "+---+-------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "despachantes.select(\"id\",\"nome\",\"vendas\").where(despachantes.vendas > 20).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "386f7c68-fb78-4642-9125-bb1403768d39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+------+\n",
      "| id|               nome|vendas|\n",
      "+---+-------------------+------+\n",
      "|  1|   Carminda Pestana|    23|\n",
      "|  2|    Deolinda Vilela|    34|\n",
      "|  3|   Emídio Dornelles|    34|\n",
      "|  4|Felisbela Dornelles|    36|\n",
      "|  6|   Matilde Rebouças|    22|\n",
      "+---+-------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "despachantes.select(\"id\",\"nome\",\"vendas\").where((despachantes.vendas > 20) & (despachantes.vendas < 40)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac99ce7-9a46-4c8a-99cf-891b26de5eff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'nomes', 'status', 'cidade', 'vendas', 'data']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novodf = despachantes.withColumnRenamed(\"nome\", \"nomes\")\n",
    "novodf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be7e67d-f337-4617-8bdf-dc6de6972a4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ac0baca-1e97-486e-96d5-010e4faa7431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- nome: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- cidade: string (nullable = true)\n",
      " |-- vendas: integer (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      " |-- data_formato_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "despachantes2 = despachantes.withColumn(\"data_formato_timestamp\", to_timestamp(despachantes.data, \"yyyy-MM-dd\"))\n",
    "despachantes2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7023c3e4-7c80-4523-b35e-f23bb4c1176f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|year(data)|\n",
      "+----------+\n",
      "|      2020|\n",
      "|      2020|\n",
      "|      2020|\n",
      "|      2020|\n",
      "|      2020|\n",
      "|      2019|\n",
      "|      2019|\n",
      "|      2020|\n",
      "|      2018|\n",
      "|      2020|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "despachantes2.select(year(\"data\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c51cab5-5384-4dd5-a2d2-614656ab135b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|year(data)|\n",
      "+----------+\n",
      "|      2018|\n",
      "|      2019|\n",
      "|      2020|\n",
      "+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "despachantes2.select(year(\"data\")).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65583cd9-511c-4a46-8c3a-ff6119b6f5e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|               nome|year(data)|\n",
      "+-------------------+----------+\n",
      "|   Carminda Pestana|      2020|\n",
      "|    Deolinda Vilela|      2020|\n",
      "|   Emídio Dornelles|      2020|\n",
      "|Felisbela Dornelles|      2020|\n",
      "|     Graça Ornellas|      2020|\n",
      "|   Matilde Rebouças|      2019|\n",
      "|    Noêmia   Orriça|      2019|\n",
      "|      Roque Vásquez|      2020|\n",
      "|      Uriel Queiroz|      2018|\n",
      "|   Viviana Sequeira|      2020|\n",
      "+-------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "despachantes2.select(\"nome\", year(\"data\")).orderBy(\"nome\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f80553-135a-4f4c-acdc-dcb8ef6a1ef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|year(data)|count|\n",
      "+----------+-----+\n",
      "|      2018|    1|\n",
      "|      2019|    2|\n",
      "|      2020|    7|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "despachantes2.select(\"data\").groupBy(year(\"data\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "642b9f11-9ba0-41e0-b99f-ac6861a0177d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(vendas)|\n",
      "+-----------+\n",
      "|        325|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "despachantes2.select(sum(\"vendas\")).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1 - Dataframes",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "spark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
